{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034347b4-d35a-46f9-9988-508ea8c938d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0154c-d987-4ea0-8e33-aca6d322ba8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC-USD\n",
      "LTC-USD\n",
      "BCH-USD\n",
      "ETH-USD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GtothaV\\AppData\\Local\\Temp\\ipykernel_10176\\349930309.py:94: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  main_df.fillna(method=\"ffill\", inplace=True)  # forward fill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (77922, 60, 8)\n",
      "Validation X shape: (3860, 60, 8)\n",
      "Train data: 77922 validation: 3860\n",
      "Don't buys: 38961, buys: 38961\n",
      "VALIDATION Don't buys: 1930, buys: 1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GtothaV\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\GtothaV\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 914/1218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m37s\u001b[0m 123ms/step - accuracy: 0.5093 - loss: 0.7555 "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "SEQ_LEN = 60  # how long of a preceding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"LTC-USD\"\n",
    "EPOCHS = 10  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "\n",
    "\n",
    "def classify(current, future):\n",
    "    return 1 if float(future) > float(current) else 0  # Buy if future price is higher\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.drop(\"future\", axis=1)  # don't need this anymore.\n",
    "\n",
    "    for col in df.columns:  # normalize all columns except the target\n",
    "        if col != \"target\":\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace=True)  # remove NaNs\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale values\n",
    "\n",
    "    df.dropna(inplace=True)  # cleanup again\n",
    "\n",
    "    sequential_data = []  # to store the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # store up to SEQ_LEN days\n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have enough days\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append the sequence\n",
    "\n",
    "    random.shuffle(sequential_data)  # shuffle for randomness\n",
    "\n",
    "    buys = []  # store buy sequences and targets\n",
    "    sells = []  # store sell sequences and targets\n",
    "\n",
    "    for seq, target in sequential_data:  # iterate through the data\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])  # not a buy\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])  # is a buy\n",
    "\n",
    "    random.shuffle(buys)  # shuffle buys and sells\n",
    "    random.shuffle(sells)\n",
    "\n",
    "    lower = min(len(buys), len(sells))  # get the shorter length\n",
    "\n",
    "    buys = buys[:lower]  # trim to the shortest length\n",
    "    sells = sells[:lower]\n",
    "\n",
    "    sequential_data = buys + sells  # combine\n",
    "    random.shuffle(sequential_data)  # final shuffle\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # create X and y\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    return np.array(X), np.array(y)  # ensure both are numpy arrays\n",
    "\n",
    "\n",
    "main_df = pd.DataFrame()  # start with an empty DataFrame\n",
    "\n",
    "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
    "for ratio in ratios:\n",
    "    print(ratio)\n",
    "    dataset = f\"C:/Users/GtothaV/Desktop/crypto_data/{ratio}.csv\"  # path to the file\n",
    "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  # read data\n",
    "\n",
    "    # rename columns to include the ticker\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
    "\n",
    "    df.set_index(\"time\", inplace=True)  # set time as index\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # keep only relevant columns\n",
    "\n",
    "    if len(main_df) == 0:  # if empty, initialize\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)  # join with main DataFrame\n",
    "\n",
    "main_df.fillna(method=\"ffill\", inplace=True)  # forward fill\n",
    "main_df.dropna(inplace=True)\n",
    "\n",
    "# Prepare future predictions\n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
    "\n",
    "main_df.dropna(inplace=True)\n",
    "\n",
    "# Split data for validation\n",
    "times = sorted(main_df.index.values)\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.05 * len(times))]\n",
    "\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "main_df = main_df[(main_df.index < last_5pct)]\n",
    "\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train X shape:\", train_x.shape)  # Should be (samples, SEQ_LEN, features)\n",
    "print(\"Validation X shape:\", validation_x.shape)  # Should be (samples, SEQ_LEN, features)\n",
    "\n",
    "# Check counts of buys and sells using numpy\n",
    "unique_train, counts_train = np.unique(train_y, return_counts=True)\n",
    "unique_validation, counts_validation = np.unique(validation_y, return_counts=True)\n",
    "\n",
    "train_counts = dict(zip(unique_train, counts_train))\n",
    "validation_counts = dict(zip(unique_validation, counts_validation))\n",
    "\n",
    "print(f\"Train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Don't buys: {train_counts.get(0, 0)}, buys: {train_counts.get(1, 0)}\")\n",
    "print(f\"VALIDATION Don't buys: {validation_counts.get(0, 0)}, buys: {validation_counts.get(1, 0)}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))  # Output layer for binary classification\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "# Define checkpoint\n",
    "filepath = \"models/RNN_Final-{epoch:02d}-{val_accuracy:.3f}.keras\"  # Save best model\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save final model\n",
    "model.save(f\"models/{NAME}.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefbe98-644c-40d7-b4da-690fe43b770e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
